<!DOCTYPE html>
<html>
<head>
        <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
        <link rel="stylesheet" type="text/css" href="styles.css" />
        <link rel="stylesheet" href="prism.css">
        <script src="prism.js"></script>

        <h1>Ky McCormick - Personal Site</h1>

        <nav>
                <a href="./index.html">[resume]</a>
                <a href="./projects.html">[projects]</a>
                <a href="./aboutme.html">[about me]</a>
                <a href="./today.html">[todays info]</a>
        </nav>
        <hr>
</head>
<body>
	<h1>Linear Discriminant Analysis</h1>
	<p>Linear Discriminant Analysis, at its heart, is a mathematical technique. Machine learning really took it soley due to its usefulness, 
	   but machine learning is truly 20% Linear Alg, 20% Stats, and 60% just getting the data where you want it. This, as the name implies,
	   comes from the linear algebra and statistics domains.
	</p>
	<h2>What is LDA?</h2>
	<p>Linear Discriminant Analysis (LDA) is a technique to reduce the dimmensionality of a dataset while preserving class separation. For problems
	   like classification LDA is essential to avoid the Curse of Dimmensionality, to increase compatibility with certain algorithms, and to visually
	   represent data. The general idea of LDA is to find a new axis that best separates two (or more) classes.
	</p>
	<h2>What are LDAs Assumptions and Drawbacks?</h2>
	<p>LDA is not a silver bullet. LDA makes some crucial assumptions about the dataset:
	</p>
	<ul>
		<li>1. Sample Independence</li>
		<li>2. Gaussian Distribution of data</li>
		<li>3. Each class has identical covariance matrices</li>
		<li>4. Linear separation is possible</li>
	</ul>
	<p>
	   most of these assumptions are fairly reasonable. The two that get us in the most trouble are 2 and 3. Not all data will be gaussian in nature.
	   And for each class to have identical covariance matrices is ideal, and is generally true, but be careful about finding if it does not.
	</p>
	<h2>The Mathematics of LDA</h2>
	<p>LDA has a couple of steps. It is important to understand each step to understand the algorithm.</p>
	<ul>
		<li>1. Find the mean over all features</li>
		<li>2. Find the scatter matrices for in-between and within-class, SB and SW</li>
		<li>3. Find the eigenvectors and values for the scatter matrices</li>
		<li>4. Choose the k eigenvectors mapping to the k largest eigenvalues</li>
		<li>5. Use these eigenvectors to transform the samples to a new, dimmensionally reduced, subspace</li>
	</ul>
	<p>
	   Great! Now that we know the steps, let's break down how to perform them:
	   <br><br>
	   Step 1: Find the mean over all features for each class
	   <br>
	   &emsp; - For each class, for each feature of each entry in that class, find it's mean over the entries in that class. <br>
	   &emsp; - Results in C 1xF matrix where F = # of Features, where C = # classes, each called m<sub>i</sub>, <br>
	   &emsp;&ensp; i being the class number.
	   <br>
	   Step 2: Find the Scatter Matrices
	   <br>
	   &emsp;- SW = the sum of Si over classes<br>
	   &emsp;- Si = the sum of (x-m<sub>i</sub>)(x-m<sub>i</sub>)<sup>T</sup><br>
	   &emsp;- SB = the sum of N(m<sub>i</sub>-m)(m<sub>i</sub>-m)<sup>T</sup> where N = sample size of class, m = total mean of feature.<br>
	   <br>
	   Step 3: Find the eigenvectors and values for the scatter matrices
	   <br>
	   &emsp;- Solve the generalized eigenvalue problem for SW inverse * SB<br>
	   <br>
	   Step 4 and 5:
	   <br>
	   &emsp;- Take the k largest and multiply Y = X x W where W = the d x k-dimmensional eigenvect matrix<br>
	</p>
	<br>
	<hr>
	<h2>LDA  Applied in Python</h2>
	<pre><code>

import numpy as np

class LDA:

    def __init__(self, n_componenets):
	# number of dimmensions to keep
	# disrciminants = store for eigenvectors 

	self.n_componenets = n_componenets
	self.discriminants = None

    def fit(self, X, y):
	num_features = X.shape[1]
	class_labels = np.unique(y)

	# calc S_W and S_B

	mean_over_all = np.mean(X,axis=0)

	SW = np.zeros((num_features,num_features))
	SB = np.zeros((num_features,num_features))

	for c in class_labels:
		X_c = X[y==c]
		mean_c = np.mean(X_c,axis=0)
		SW += (X_c - mean_c).T.dot(X_c - mean_c)

		n_C = X_c.shape[0]
		mean_diff = (mean_c - mean_over_all).reshape(num_features,1)
		SB += n_C * (mean_diff).dot(mean_diff.T)

	A = np.linalg.inv(SW).dot(SB)
	eigenvalues, eigenvectors = np.linalg.eig(A)
	eigenvectors = eigenvectors.T
	idxs = np.argsort(abs(eigenvalues))[::-1]
	eigenvalues = eigenvalues[idxs]
	eigenvectors = eigenvectors[idxs]

	self.discriminants = eigenvectors[0:self.n_componenets]

    def transform(self, X):
	# project down dimmensions

	return np.dot(X, self.discriminants.T)
	</code></pre>
</body>
</html>
