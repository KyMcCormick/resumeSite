<!DOCTYPE html>
<html>
<head>
        <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
        <link rel="stylesheet" type="text/css" href="styles.css" />

        <h1>Ky McCormick - Personal Site</h1>

        <nav>
                <a href="./index.html">[resume]</a>
                <a href="./projects.html">[projects]</a>
                <a href="./aboutme.html">[about me]</a>
                <a href="./today.html">[todays info]</a>
        </nav>
</head>
<body>
	<title>Ky McCormick Personal Site</title>
	<hr>
	<h2>Wikipedia Scraper Lab</h2>
	<p>Welcome! Here we will learn how to create the wikipedia scraper found on the <br>
	todays info section of this website. Starting this project, we have a couple of goals <br>
	in mind. We want to learn more about BeautifulSoup and Requests modules, which are used <br>
	in the webscraping process. We want to learn more behind the structure of html pages and <br>
	automatisation that can be done there. Any self respecing computer scientist does not look <br>
	to create more work where less work works just as well. We also want to to tie this to startup.</p> 
	<hr>
	<p> From above we can pull the following broad questions </p>
	<ul>
		<li> 1. How does web scraping work? </li>
		<li> 2. How can I make this as automated as possible </li>
		<li> 3. What tools and processes are involved </li>
		<li> 4. What steps can I take to efficiently make my script generic? </li>
	</ul>
	<p> As we continue, these broad questions will be brokenb down with each sub-question <br>
	labeled 1.a, 2.b, etc. <p>
	<hr>
	
	<p> Alright. Let us start with defining our overarching problem. </p> 
	<p>1.a What is web scraping? <br> 
	<p style="margin-left:20px">
		Web Scraping is the act of extracting data from a site and then using that data <br>
		is some way. Most commonly, this extraction is of plain html elements, such as prices <br>
		text, or list elements, however one can also extract metadata from pages.
	</p>
	
	<p> This leads us to our next question. We've defined web scraping, but this has left us with another <br>
	topic to define. This is a very common occurance in cs, when we peel back a layer of abstraction, we get more questions </p>
	
	<p>1.b What structure is involved in a web page?</p>
	<p style="margin-left:20px"> 
		Below is an snipit of the html for wikipedia's December 21 page. This is the juicy information. <br>
		html, along with javascript and css, form the basis for every website on the internet. html <br>
		provides the structure and content, css the layour and pretification, and javascript interactivity <br>
		and functionality. The easisiest way to look at html is to inspect element. We are going to focus <br>
		on the li tags, these are the lists of events per era. </p>
	<img src="./wikipediaImages/wikipedia.PNG">

	<p> All of that is great, and a lot of information. If we are to do anything with this we must get it into <br>
	our program which, well, that's a good question. How does tsome program that lives on your <br>
	computer and/or github get info from a website that may be hosted on a server halfway across the world? </p>

	<p> 2/3.a How do we get this info to our program? </p>
	<p style="margin-left:20px">
	This is where requests come in. http is a lovely little thing we take for granted and most cs students just overlook, but
	using the requests module, which is built on top of older http libraries, we can get all the info we need. The request module
	is so popular, there is currently a push to get it included in python by default. We can create a request object that gets
	the page for the current date. We'll come back to the question of the date later, but for now let's say it is hardcoded as 
	December_13. <br> <br>
	We do not (sadly) need to get into the weeds on how http works, but here is a simple explanation. 
	http allows us to communicate requests and responces between two places, call one a client, who requests, and a server,
	who responds. When we connect to some website, that site is really hosted somewhere on some webserver, 
	and through TCP and IP (transport and internet layers) we get there and have a connection. Great. http
	then allows for us to actually send a representation the current state of the site from the server to the client so we see
	all the pretty text and images and all the rest. When we do this with the request module, we are sending a GET request
	that will retrieve all the data aka html and css nonsense. Notably, we also want ensure the webpage we get is ok, so we check
	for status code 200, the ok signal that servers send the client after a successful GET.
	</p>
	<p> Now that we know it is possible to get this information and check that it is correct, then we can move to our design.
	It is imperitive to think about the design and structure of a program before starting. We think both about the problem itself
	and the structure behind the problem and we think about the structure of our solutions. </p>

	<p> 4.a What structurly do I expect to need in the code? </p>
	<p style="margin-left:20px">
	This is a large question, a shockingly large and important question. We know that a couple steps are needed. Broadly,
	get date, get data, parse data, and get events. 
	<ul>
		<li>getDate(): fairly simple, nearly all computers have clocks (or well, all if you count the clock unit). Getting
		the date is relatively trivial, we just then need to concatenate the right form of it to "https://en.wikipedia.org/wiki
		<li>getSiteData(): this is the big boy, main driver. Once we get the date, we can make the requester, send an GET request
		and then, if we have a 200 response, begin to process
		<li>processData(): We need to parse the data we receive as html that way it is in a useable format instead of a long string
		and then pull all text out of it, and work the document's sections. The wikipedia pages we are scraping are divided
		into four sections; events, births, deaths, and holidays. Each of these parts are broken into three subparts,
		pre1600, early modern period, and modern period. This logically means we should have another function.
		<li>processSection(): For each section, we can separate the subsections as lists of facts contained within. 
		<li>getRandomEvents(): a simple function get random events from the subsections.
	</ul>
	</p>
	<p>This gives us a rough idea of the intitial tasks we will need. With this, we now can begin coding. Here is our initial program.
	Note  the imports at the top. Of these, requests is needed for the http requests and bs4 from BeautifulSoup is needed for parsing
	the html.
	<img src="./wikipediaImages/initialCodeStructure.PNG"> 
	<br>
	let's start with getSiteData(): first since this function is the entry point of our program. Firstly, we know we need to get
	the current date, so call the getDate() function, and execute a request, afterwhich we check the response code. After this, we should
	be good to process. The function getDate is fairly simple to implement as well. 
	<img src="./wikipediaImages/getSiteData.PNG">
	Next in the pipeline is processData(requestor). Remember that knowing the structure of the website being scraped is imperative.
	With this in mind, we have a really important question.
</body>
</html>
