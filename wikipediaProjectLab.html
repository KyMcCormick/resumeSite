<!DOCTYPE html>
<html>
<head>
        <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
        <link rel="stylesheet" type="text/css" href="styles.css" />

        <h1>Ky McCormick - Personal Site</h1>

        <nav>
                <a href="./index.html">[resume]</a>
                <a href="./projects.html">[projects]</a>
                <a href="./aboutme.html">[about me]</a>
                <a href="./today.html">[todays info]</a>
        </nav>
</head>
<body>
	<title>Ky McCormick Personal Site</title>
	<hr>
	<h2>Wikipedia Scraper Lab</h2>
	<p>Welcome! Here we will learn how to create the wikipedia scraper found on the <br>
	todays info section of this website. Starting this project, we have a couple of goals <br>
	in mind. We want to learn more about BeautifulSoup and Requests modules, which are used <br>
	in the webscraping process. We want to learn more behind the structure of html pages and <br>
	automatisation that can be done there. Any self respecing computer scientist does not look <br>
	to create more work where less work works just as well. We also want to to tie this to startup.</p> 
	<hr>
	<p> From above we can pull the following broad questions </p>
	<ul>
		<li> 1. How does web scraping work? </li>
		<li> 2. How can I make this as automated as possible </li>
		<li> 3. What tools and processes are involved </li>
		<li> 4. What steps can I take to efficiently make my script generic? </li>
	</ul>
	<p> As we continue, these broad questions will be brokenb down with each sub-question <br>
	labeled 1.a, 2.b, etc. <p>
	<hr>
	
	<p> Alright. Let us start with defining our overarching problem. </p> 
	<p>1.a What is web scraping? <br> 
	<p style="margin-left:20px">
		Web Scraping is the act of extracting data from a site and then using that data <br>
		is some way. Most commonly, this extraction is of plain html elements, such as prices <br>
		text, or list elements, however one can also extract metadata from pages.
	</p>
	
	<p> This leads us to our next question. We've defined web scraping, but this has left us with another <br>
	topic to define. This is a very common occurance in cs, when we peel back a layer of abstraction, we get more questions </p>
	
	<p>1.b What structure is involved in a web page?</p>
	<p style="margin-left:20px"> 
		Below is an snipit of the html for wikipedia's December 21 page. This is the juicy information. <br>
		html, along with javascript and css, form the basis for every website on the internet. html <br>
		provides the structure and content, css the layour and pretification, and javascript interactivity <br>
		and functionality. The easisiest way to look at html is to inspect element. We are going to focus <br>
		on the li tags, these are the lists of events per era. </p>
	<img src="./wikipediaImages/wikipedia.PNG">

	<p> All of that is great, and a lot of information. If we are to do anything with this we must get it into <br>
	our program which, well, that's a good question. How does tsome program that lives on your <br>
	computer and/or github get info from a website that may be hosted on a server halfway across the world? </p>

	<p> 2/3.a How do we get this info to our program? </p>
	<p style="margin-left:20px">
	This is where requests come in. http is a lovely little thing we take for granted and most cs students just overlook, but
	using the requests module, which is built on top of older http libraries, we can get all the info we need. The request module
	is so popular, there is currently a push to get it included in python by default. We can create a request object that gets
	the page for the current date. We'll come back to the question of the date later, but for now let's say it is hardcoded as 
	December_13. <br> <br>
	We do not (sadly) need to get into the weeds on how http works, but here is a simple explanation. 
	http allows us to communicate requests and responces between two places, call one a client, who requests, and a server,
	who responds. When we connect to some website, that site is really hosted somewhere on some webserver, 
	and through TCP and IP (transport and internet layers) we get there and have a connection. Great. http
	then allows for us to actually send a representation the current state of the site from the server to the client so we see
	all the pretty text and images and all the rest. When we do this with the request module, we are sending a GET request
	that will retrieve all the data aka html and css nonsense. Notably, we also want ensure the webpage we get is ok, so we check
	for status code 200, the ok signal that servers send the client after a successful GET.
	</p>
</body>
</html>
